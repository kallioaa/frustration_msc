{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb4226e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(str(Path.cwd().resolve().parent.parent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c7e8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  experiments\n",
    "from experiments.run_sweep import run_sweep\n",
    "from experiments.run_experiment import TrainingConfig, EvaluateConfig\n",
    "from experiments.sweep_plots import plot_sweep_training, plot_sweep_evaluation\n",
    "from experiments.sweep_plots_helper import cliffwalking_training_plot_specs, cliffwalking_evaluation_plot_specs\n",
    "from experiments.notebook_helpers import generate_alpha_pairs, asymmetric_alphas\n",
    "\n",
    "\n",
    "# environments\n",
    "from environments.cliffwalking import CliffWalkingConfig, get_cliffwalking_env\n",
    "\n",
    "# SARSA confirmation bias agent\n",
    "from agents.sarsa_td0_confirmation_bias import (\n",
    "    SarsaTD0ConfirmationBiasConfig,\n",
    "    SarsaTD0ConfirmationBiasAgent,\n",
    ")\n",
    "\n",
    "# metrics for training\n",
    "from metrics.reward_mertrics import cliffwalking_reward_metrics_specs\n",
    "from metrics.frustration_metrics import frustration_metrics_specs\n",
    "\n",
    "# external libraries\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566c95d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_AGENT_CONFIG_CONF = dict(\n",
    "    alpha_conf=0.2,\n",
    "    alpha_disconf=0.2,\n",
    "    gamma=0.99,\n",
    "    epsilon=0.2,\n",
    "    reward_metrics=cliffwalking_reward_metrics_specs(),\n",
    "    td_error_metrics=frustration_metrics_specs(),\n",
    ")\n",
    "\n",
    "BASE_AGENT_CONFIG_POSITITY = dict(\n",
    "    alpha_positive=0.2,\n",
    "    alpha_negative=0.2,\n",
    "    gamma=0.99,\n",
    "    epsilon=0.2,\n",
    "    reward_metrics=cliffwalking_reward_metrics_specs(),\n",
    "    td_error_metrics=frustration_metrics_specs(),\n",
    ")\n",
    "\n",
    "NUM_TRAIN_EPISODES = 10000\n",
    "NUM_EVAL_EPISODES = 2000\n",
    "NUM_SEEDS = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eded1d62",
   "metadata": {},
   "source": [
    "## Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "774859a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_config = CliffWalkingConfig(is_slippery=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc60c7b",
   "metadata": {},
   "source": [
    "## Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "060ad17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Agents\n",
    "# sarsa_td0 confirmation bias agent\n",
    "agent_factory = SarsaTD0ConfirmationBiasAgent\n",
    "sarsa_td0_config = SarsaTD0ConfirmationBiasConfig(**BASE_AGENT_CONFIG_CONF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0275606c",
   "metadata": {},
   "source": [
    "## Sweep configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764c75c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_train = TrainingConfig(\n",
    "    name=\"CliffWalking_sarsa_td0\",\n",
    "    num_train_episodes=NUM_TRAIN_EPISODES,\n",
    "    env_kwargs={\"config\": env_config},\n",
    "    agent_kwargs={\"config\": sarsa_td0_config},\n",
    ")\n",
    "\n",
    "base_eval = EvaluateConfig(\n",
    "    name=\"CliffWalking_sarsa_td0\",\n",
    "    num_eval_episodes=NUM_EVAL_EPISODES,  # use >0 if you want eval outputs\n",
    "    env_kwargs={\"config\": env_config},\n",
    "    evaluation_metrics=cliffwalking_reward_metrics_specs(),\n",
    "    td_error_metrics=frustration_metrics_specs(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579c3f8b",
   "metadata": {},
   "source": [
    "## Confirmation bias results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b963a3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# different q_tables\n",
    "env = get_cliffwalking_env(env_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1618ec22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running CliffWalking pair=(0.325, 0.075) with 15 seeds...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 58\u001b[0m\n\u001b[1;32m     41\u001b[0m sweep_one_pair \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magent_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[1;32m     43\u001b[0m         {\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     52\u001b[0m     ],\n\u001b[1;32m     53\u001b[0m }\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning CliffWalking pair=(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ma_conf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00ma_disconf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(seeds)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seeds...\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     57\u001b[0m )\n\u001b[0;32m---> 58\u001b[0m pair_results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_sweep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_training\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_evaluation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43msweep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msweep_one_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv_factory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_cliffwalking_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43magent_factory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43magent_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# save immediately\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m out_file\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m~/frustration_msc/src/experiments/run_sweep.py:66\u001b[0m, in \u001b[0;36mrun_sweep\u001b[0;34m(base_training, base_evaluation, sweep, env_factory, agent_factory)\u001b[0m\n\u001b[1;32m     63\u001b[0m training_config \u001b[38;5;241m=\u001b[39m TrainingConfig(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtraining_dict)\n\u001b[1;32m     64\u001b[0m evaluation_config \u001b[38;5;241m=\u001b[39m EvaluateConfig(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mevaluation_dict)\n\u001b[0;32m---> 66\u001b[0m agent, training_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mrun_training\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv_factory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv_factory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent_factory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43magent_factory\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m eval_metrics \u001b[38;5;241m=\u001b[39m run_evaluation(\n\u001b[1;32m     70\u001b[0m     evaluation_config, env_factory\u001b[38;5;241m=\u001b[39menv_factory, agent\u001b[38;5;241m=\u001b[39magent\n\u001b[1;32m     71\u001b[0m )\n\u001b[1;32m     73\u001b[0m results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m     74\u001b[0m     {\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m\"\u001b[39m: override,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     78\u001b[0m     }\n\u001b[1;32m     79\u001b[0m )\n",
      "File \u001b[0;32m~/frustration_msc/src/experiments/run_experiment.py:56\u001b[0m, in \u001b[0;36mrun_training\u001b[0;34m(config, env_factory, agent_factory)\u001b[0m\n\u001b[1;32m     54\u001b[0m train_signature \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(agent\u001b[38;5;241m.\u001b[39mtrain)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_env_factory\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m train_signature\u001b[38;5;241m.\u001b[39mparameters:\n\u001b[0;32m---> 56\u001b[0m     training_metrics \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_train_episodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_env_factory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv_factory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_env_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     63\u001b[0m     training_metrics \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mtrain(env, num_episodes\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_train_episodes)\n",
      "File \u001b[0;32m~/frustration_msc/src/agents/sarsa_td0_confirmation_bias.py:244\u001b[0m, in \u001b[0;36mSarsaTD0ConfirmationBiasAgent.train\u001b[0;34m(self, env, num_episodes, eval_env_factory, eval_env_kwargs)\u001b[0m\n\u001b[1;32m    235\u001b[0m td_error_metrics_log[\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisconfirmatory_nongreedy_positive_ratio_per_episode\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    237\u001b[0m ]\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(disconfirmatory_nongreedy_positive_ratio))\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    240\u001b[0m     eval_env_factory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39meval_every_episodes \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (episode_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39meval_every_episodes \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    243\u001b[0m ):\n\u001b[0;32m--> 244\u001b[0m     checkpoint_metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_mid_eval\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_env_factory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_env_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_env_kwargs\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    247\u001b[0m     checkpoint_metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepisode\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(episode_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    248\u001b[0m     mid_eval_log\u001b[38;5;241m.\u001b[39mappend(checkpoint_metrics)\n",
      "File \u001b[0;32m~/frustration_msc/src/agents/sarsa_td0_confirmation_bias.py:60\u001b[0m, in \u001b[0;36mSarsaTD0ConfirmationBiasAgent._run_mid_eval\u001b[0;34m(self, eval_env_factory, eval_env_kwargs)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_run_mid_eval\u001b[39m(\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     56\u001b[0m     eval_env_factory: Callable[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, Any],\n\u001b[1;32m     57\u001b[0m     eval_env_kwargs: Optional[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     58\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m     59\u001b[0m     eval_env_kwargs \u001b[38;5;241m=\u001b[39m eval_env_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[0;32m---> 60\u001b[0m     env \u001b[38;5;241m=\u001b[39m \u001b[43meval_env_factory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43meval_env_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     env \u001b[38;5;241m=\u001b[39m TimeLimit(env, max_episode_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmax_eval_steps)\n\u001b[1;32m     63\u001b[0m     reward_metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mreward_metrics \u001b[38;5;129;01mor\u001b[39;00m {}\n",
      "File \u001b[0;32m~/frustration_msc/src/environments/cliffwalking.py:21\u001b[0m, in \u001b[0;36mget_cliffwalking_env\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m     20\u001b[0m     config \u001b[38;5;241m=\u001b[39m CliffWalkingConfig(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig)\n\u001b[0;32m---> 21\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mgym\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCliffWalking-v1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_slippery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_slippery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mseed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     23\u001b[0m     env\u001b[38;5;241m.\u001b[39mreset(seed\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mseed)\n",
      "File \u001b[0;32m~/frustration_msc/.venv/lib/python3.12/site-packages/gymnasium/envs/registration.py:734\u001b[0m, in \u001b[0;36mmake\u001b[0;34m(id, max_episode_steps, disable_env_checker, **kwargs)\u001b[0m\n\u001b[1;32m    728\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    729\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe environment is being initialised with render_mode=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrender_mode\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    730\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthat is not in the possible render_modes (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrender_modes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    731\u001b[0m         )\n\u001b[1;32m    733\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 734\u001b[0m     env \u001b[38;5;241m=\u001b[39m \u001b[43menv_creator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43menv_spec_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    737\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot an unexpected keyword argument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrender_mode\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    738\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m apply_human_rendering\n\u001b[1;32m    739\u001b[0m     ):\n",
      "File \u001b[0;32m~/frustration_msc/.venv/lib/python3.12/site-packages/gymnasium/envs/toy_text/cliffwalking.py:124\u001b[0m, in \u001b[0;36mCliffWalkingEnv.__init__\u001b[0;34m(self, render_mode, is_slippery)\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mP[s][UP] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_calculate_transition_prob(position, UP)\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mP[s][RIGHT] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_calculate_transition_prob(position, RIGHT)\n\u001b[0;32m--> 124\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mP[s][DOWN] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_calculate_transition_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mposition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDOWN\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mP[s][LEFT] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_calculate_transition_prob(position, LEFT)\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# Calculate initial state distribution\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# We always start in state (3, 0)\u001b[39;00m\n",
      "File \u001b[0;32m~/frustration_msc/.venv/lib/python3.12/site-packages/gymnasium/envs/toy_text/cliffwalking.py:185\u001b[0m, in \u001b[0;36mCliffWalkingEnv._calculate_transition_prob\u001b[0;34m(self, current, move)\u001b[0m\n\u001b[1;32m    183\u001b[0m new_position \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(current) \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39marray(delta)\n\u001b[1;32m    184\u001b[0m new_position \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_limit_coordinates(new_position)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m--> 185\u001b[0m new_state \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mravel_multi_index\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_position\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cliff[\u001b[38;5;28mtuple\u001b[39m(new_position)]:\n\u001b[1;32m    187\u001b[0m     outcomes\u001b[38;5;241m.\u001b[39mappend((\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(deltas), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_state_index, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# create different initial q_tables for the sweep\n",
    "num_states = env.observation_space.n\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "out_dir = Path(\"outputs/sweeps\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# q-table setup\n",
    "q0 = np.zeros((num_states, num_actions), dtype=np.float64)\n",
    "q_tables = [(\"zeros\", q0)]\n",
    "\n",
    "# confirmation-bias alpha pairs\n",
    "confirmatory_pairs, balanced_pairs, disconfirmatory_pairs = generate_alpha_pairs(\n",
    "    balanced_lr=[0.2], num_pairs=5, step_size=0.025\n",
    ")\n",
    "\n",
    "# per your current setup: only balanced\n",
    "alpha_pairs = confirmatory_pairs + balanced_pairs + disconfirmatory_pairs\n",
    "seeds = list(range(NUM_SEEDS))\n",
    "\n",
    "all_results = []\n",
    "done_pairs = []\n",
    "\n",
    "for a_conf, a_disconf in alpha_pairs:\n",
    "    # safe file name for float values\n",
    "    pair_tag = f\"ac_{a_conf:.3f}_ad_{a_disconf:.3f}\".replace(\".\", \"p\")\n",
    "    out_file = out_dir / f\"cliffwalking_conf_slippery{pair_tag}.pkl\"\n",
    "\n",
    "    # resume behavior\n",
    "    if out_file.exists():\n",
    "        print(f\"Skipping pair=({a_conf}, {a_disconf}) (already exists): {out_file}\")\n",
    "        with out_file.open(\"rb\") as f:\n",
    "            pair_results = pickle.load(f)\n",
    "        all_results.extend(pair_results)\n",
    "        done_pairs.append((a_conf, a_disconf))\n",
    "        continue\n",
    "\n",
    "    sweep_one_pair = {\n",
    "        \"agent_kwargs\": [\n",
    "            {\n",
    "                \"alpha_conf\": a_conf,\n",
    "                \"alpha_disconf\": a_disconf,\n",
    "                \"seed\": seed,\n",
    "                \"initial_q_table\": q_table,\n",
    "                \"initial_q_table_label\": label,\n",
    "            }\n",
    "            for (label, q_table) in q_tables\n",
    "            for seed in seeds\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    print(\n",
    "        f\"Running CliffWalking pair=({a_conf}, {a_disconf}) with {len(seeds)} seeds...\"\n",
    "    )\n",
    "    pair_results = run_sweep(\n",
    "        base_training=base_train,\n",
    "        base_evaluation=base_eval,\n",
    "        sweep=sweep_one_pair,\n",
    "        env_factory=get_cliffwalking_env,\n",
    "        agent_factory=agent_factory,\n",
    "    )\n",
    "\n",
    "    # save immediately\n",
    "    with out_file.open(\"wb\") as f:\n",
    "        pickle.dump(pair_results, f)\n",
    "\n",
    "    all_results.extend(pair_results)\n",
    "    done_pairs.append((a_conf, a_disconf))\n",
    "    print(f\"Saved pair=({a_conf}, {a_disconf}) -> {out_file}\")\n",
    "\n",
    "# optional combined file\n",
    "combined_file = out_dir / \"cliffwalking_conf_slippery_all_pairs.pkl\"\n",
    "with combined_file.open(\"wb\") as f:\n",
    "     pickle.dump(all_results, f)\n",
    "\n",
    "print(f\"Done pairs: {len(done_pairs)} / {len(alpha_pairs)}\")\n",
    "print(f\"Total runs collected: {len(all_results)}\")\n",
    "# print(f\"Combined saved to: {combined_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539bc054",
   "metadata": {},
   "source": [
    "## Positivity bias results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5e88625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SARSA positivty bias agent\n",
    "from agents.sarsa_td0_positivity_bias import (\n",
    "    SarsaTD0PositivityBiasConfig,\n",
    "    SarsaTD0PositivityBiasAgent,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde3c6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Agents\n",
    "# sarsa_td0 agent\n",
    "agent_factory = SarsaTD0PositivityBiasAgent\n",
    "sarsa_td0_config = SarsaTD0PositivityBiasConfig(**BASE_AGENT_CONFIG_POSITITY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a983e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_train = TrainingConfig(\n",
    "    name=\"CliffWalking_sarsa_td0\",\n",
    "    num_train_episodes=NUM_TRAIN_EPISODES,\n",
    "    env_kwargs={\"config\": env_config},\n",
    "    agent_kwargs={\"config\": sarsa_td0_config},\n",
    ")\n",
    "\n",
    "base_eval = EvaluateConfig(\n",
    "    name=\"CliffWalking_sarsa_td0\",\n",
    "    num_eval_episodes=NUM_EVAL_EPISODES,\n",
    "    env_kwargs={\"config\": env_config},\n",
    "    evaluation_metrics=cliffwalking_reward_metrics_specs(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c4d364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# different q_tables\n",
    "env = get_cliffwalking_env(env_config)\n",
    "num_states = env.observation_space.n\n",
    "num_actions = env.action_space.n\n",
    "q0 = np.zeros((num_states, num_actions), dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7b2f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running CliffWalking ratio=0.5 with 15 seeds...\n",
      "Saved ratio=0.5 -> outputs/sweeps/cliffwalking_pos_ratio_0p5.pkl\n",
      "Running CliffWalking ratio=0.75 with 15 seeds...\n",
      "Saved ratio=0.75 -> outputs/sweeps/cliffwalking_pos_ratio_0p75.pkl\n",
      "Running CliffWalking ratio=1.0 with 15 seeds...\n",
      "Saved ratio=1.0 -> outputs/sweeps/cliffwalking_pos_ratio_1p0.pkl\n",
      "Running CliffWalking ratio=1.5 with 15 seeds...\n",
      "Saved ratio=1.5 -> outputs/sweeps/cliffwalking_pos_ratio_1p5.pkl\n",
      "Running CliffWalking ratio=2.0 with 15 seeds...\n",
      "Saved ratio=2.0 -> outputs/sweeps/cliffwalking_pos_ratio_2p0.pkl\n",
      "Running CliffWalking ratio=4.0 with 15 seeds...\n",
      "Saved ratio=4.0 -> outputs/sweeps/cliffwalking_pos_ratio_4p0.pkl\n",
      "Done ratios: 6 / 6\n",
      "Total runs collected: 90\n"
     ]
    }
   ],
   "source": [
    "# create different initial q_tables for the sweep\n",
    "num_states = env.observation_space.n\n",
    "num_actions = env.action_space.n\n",
    "\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "out_dir = Path(\"outputs/sweeps\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# q-table setup\n",
    "q0 = np.zeros((num_states, num_actions), dtype=np.float64)\n",
    "q_tables = [(\"zeros\", q0)]\n",
    "\n",
    "# positivity-bias ratios\n",
    "a_0 = 0.2\n",
    "ratios = [4.0, 2.0, 1.5, 1.0, 0.75, 0.5]\n",
    "seeds = list(range(NUM_SEEDS))\n",
    "\n",
    "all_results = []\n",
    "done_ratios = []\n",
    "\n",
    "for r in ratios:\n",
    "    ratio_tag = str(r).replace(\".\", \"p\")\n",
    "    out_file = out_dir / f\"cliffwalking_pos_slippery_ratio_{ratio_tag}.pkl\"\n",
    "\n",
    "    # resume behavior\n",
    "    if out_file.exists():\n",
    "        print(f\"Skipping ratio={r} (already exists): {out_file}\")\n",
    "        with out_file.open(\"rb\") as f:\n",
    "            ratio_results = pickle.load(f)\n",
    "        all_results.extend(ratio_results)\n",
    "        done_ratios.append(r)\n",
    "        continue\n",
    "\n",
    "    sweep_one_ratio = {\n",
    "        \"agent_kwargs\": [\n",
    "            {\n",
    "                **asymmetric_alphas(a_0, r),  # gives alpha_positive / alpha_negative\n",
    "                \"seed\": seed,\n",
    "                \"initial_q_table\": q_table,\n",
    "                \"initial_q_table_label\": label,\n",
    "            }\n",
    "            for (label, q_table) in q_tables\n",
    "            for seed in seeds\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    print(f\"Running CliffWalking ratio={r} with {len(seeds)} seeds...\")\n",
    "    ratio_results = run_sweep(\n",
    "        base_training=base_train,\n",
    "        base_evaluation=base_eval,\n",
    "        sweep=sweep_one_ratio,\n",
    "        env_factory=get_cliffwalking_env,\n",
    "        agent_factory=agent_factory,\n",
    "    )\n",
    "\n",
    "    # save immediately\n",
    "    with out_file.open(\"wb\") as f:\n",
    "        pickle.dump(ratio_results, f)\n",
    "\n",
    "    all_results.extend(ratio_results)\n",
    "    done_ratios.append(r)\n",
    "    print(f\"Saved ratio={r} -> {out_file}\")\n",
    "\n",
    "# optional combined file\n",
    "combined_file = out_dir / \"cliffwalking_pos_slippery_all_ratios.pkl\"\n",
    "with combined_file.open(\"wb\") as f:\n",
    "     pickle.dump(all_results, f)\n",
    "\n",
    "print(f\"Done ratios: {len(done_ratios)} / {len(ratios)}\")\n",
    "print(f\"Total runs collected: {len(all_results)}\")\n",
    "# print(f\"Combined saved to: {combined_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
